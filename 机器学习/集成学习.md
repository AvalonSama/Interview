# 集成学习

</br>

#### 基本思想

通过**构建并结合多个学习器**来完成学习任务，组合成一个性能更好的学习器，有时也叫多分类器系统。

> [!TIP|label:集成学习为什么有效？]
> 不同的模型通常会在测试集上产生不同的误差。平均上，集成模型能至少与其任一成员表现一致；并且如果成员的误差是独立的，集成模型将显著地比其成员表现更好。

</br>

#### 学习策略

- 对于训练集数据，通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。也就是说，集成学习有两个主要的问题需要解决，第一是**如何得到若干个个体学习器**，第二是**如何选择一种结合策略**，将这些个体学习器集合成一个强学习器。
- 第一种就是所有的个体学习器都是一个种类的，或者说是**同质**的。比如都是决策树或者神经网络个体学习器。
- 第二种是所有的个体学习器不全是一个种类的，或者说是**异质**的。
- 目前来说，同质个体学习器的应用是最广泛的，一般常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。
- 同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类：
  1. 个体学习器之间**存在强依赖关系**，一系列个体学习器基本都需要串行生成，代表算法是Boosting系列算法；
  2. 个体学习器之间**不存在强依赖关系**，一系列个体学习器可以并行生成，代表算法是Bagging和随机森林（Random Forest）系列算法。

</br>

#### 集成学习有哪些基本步骤

集成学习一般可分为以下3个步骤：
1. 找到误差相互独立的基分类器；
2. 训练基分类器；
3. 合并基分类器的结果。

</br>

#### 常见的结合策略

- 平均法
  1. 对于数值类**回归预测问题**，最常见的结合策略是平均法，分为：简单平均法和加权平均法。
  2. 加权平均法的权重是每个个体学习器的权重。
  3. 在个体学习器性能相差较大时宜采用加权平均法，性能相近时宜采用简单平均法。
- 投票法
  1. 对于**分类预测问题**，通常使用的是投票法。
  2. **相对多数投票法**，也就是少数服从多数，即`T`个弱学习器的对样本$x$的预测结果中，数量最多的类别$c_i$为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。
  3. **绝对多数投票法**，也就是票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数，否则会拒绝预测。
  4. **加权投票法**，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。
- 学习法
  1. 代表方法是Stacking，当使用Stacking的结合策略时，不是对弱学习器的结果做简单的逻辑处理，而是**再加上一层学习器**。也就是说，将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。
  2. 在这种情况下，将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。

</br>

#### Boosting方法

- 基于**串行策略**</br>
  基学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。
- 基本思路
  1. 先从初始训练集训练一个基学习器；初始训练集中各样本的权重是相同的；
  2. 根据上一个基学习器的表现，**调整样本权重**，使分类错误的样本得到更多的关注；
  3. 基于调整后的样本分布，训练下一个基学习器；
  4. 测试时，对各基学习器加权得到最终结果。
- 特点是每次学习都会使用全部训练样本
- 代表算法包括AdaBoost、GBDT、XGBoost。
- 两个基本问题
  1. 每一轮如何改变数据的权值或概率分布？
  2. 如何将弱分类器组合成一个强分类器？

</br>

#### Bagging方法

- 基于**并行策略**</br>
  基学习器之间不存在依赖关系，可同时生成。
- 基本思路
  1. 利用自助采样法对训练集随机采样，重复进行`T`次；
  2. 基于每个采样集训练一个基学习器，并得到`T`个基学习器；
  3. 预测时，集体投票决策。
- 自助采样法：对`m`个样本的训练集，有放回的采样`m`次；此时，样本在`m`次采样中始终没被采样的概率约为`0.368`，即每次自助采样只能采样到全部样本的`63%`左右。
- 特点是训练每个基学习器时**只使用一部分样本**，并且偏好使用**不稳定的学习器**作为基学习器（所谓不稳定的学习器，指的是对样本分布较为敏感的学习器）。
- 代表算法包括随机森林。

</br>

#### Stacking方法简述

- 基于**串行策略**</br>
  初级学习器与次级学习器之间存在依赖关系，初级学习器的输出作为次级学习器的输入。
- 基本思路：
  1. 先从初始训练集训练`T`个不同的初级学习器；
  2. 利用每个初级学习器的输出构建一个次级数据集，该数据集依然使用初始数据集的标签；
  3. 据新的数据集训练次级学习器；
  4. 多级学习器的构建过程类似。
- Stacking方法也可以作为一种**结合策略**，比如加权平均和投票都属于结合策略。
- 特点是为了降低过拟合的风险，一般会利用**交叉验证**的方法使不同的初级学习器在不完全相同的子集上训练。

</br>

#### 为什么使用决策树作为基学习器

- 决策树的表达能力和泛化能力，可以通过**剪枝**（调节树的层次）快速调整。
- 决策树可以方便地将**样本的权重**整合到训练过程中，而不需要使用过采样的方式来调整样本权重（可以通过调整样本损失，或者影响分枝时样本的信息增益或基尼指数来实现）。
- 决策树是一种**不稳定**的学习器。所谓不稳定，指的是数据样本的扰动会对决策树的结果产生较大的影响。

</br>

#### 为什么不稳定的学习器更适合作为基学习器

- 不稳定的学习器**容易受到样本分布的影响**（方差大），很好的引入了**随机性**；这有助于在集成学习（特别是采用Bagging策略）中**提升模型的泛化能力**。
- 为了更好的引入随机性，有时会随机选择一个属性子集中的最优分裂属性，而不是全局最优（随机森林）。

</br>

#### 还有哪些模型也适合作为基学习器

**神经网络**也属于不稳定的学习器。通过调整神经元的数量、网络层数，连接方式初始权重也能很好的引入随机性和改变模型的表达能力和泛化能力。

</br>

#### 能否使用线性分类器作为基学习器

- Bagging 方法中不推荐
  1. 线性分类器都属于稳定的学习器（方差小），对数据不敏感；
  2. 甚至可能因为Bagging的采样，导致在训练中难以收敛，增大集成分类器的偏差。
- Boosting 方法中可以使用
  1. Boosting方法主要通过**降低偏差**的方式来提升模型的性能，而线性分类器本身具有方差小的特点；
  2. XGBoost中就支持以线性分类器作为基学习器。

</br>

#### 使用强分类器作为基学习器的弊端

- 第一次产生的分类器错误率已经达到了极限，后面的分类器起不到效果，根本原因是即使改变样本权重，分类器的差异性依然太小。
- 多个分类器基本处于相似位置的超平面，达不到组合变强的效果。

</br>

#### 偏差和方差

- 简单来说，Boosting能提升弱分类器性能的原因是**降低了偏差**，Bagging则是**降低了方差**。
- Boosting的基本思路就是在不断减小模型的训练误差（拟合残差或者加大错类的权重），加强模型的学习能力，从而减小偏差。但Boosting不会显著降低方差，因为其训练过程中各基学习器是强相关的，缺少独立性。
- Bagging方法对`n`个独立不相关的模型预测结果取平均，方差是原来的`1/n`。假设所有基分类器出错的概率是独立的，超过半数基分类器出错的概率会随着基分类器的数量增加而下降。

</br>

#### AdaBoost如何解决Boosting的两个基本问题

- 每一轮如何改变数据的权值或概率分布？</br>
  开始时，每个样本的权值是一样的，AdaBoost的做法是**提高上一轮弱分类器错误分类样本的权值**，同时降低那些被正确分类样本的权值。
- 如何将弱分类器组合成一个强分类器？</br>
  AdaBoost**采取加权表决的方法**。具体的，AdaBoost会加大分类误差率小的基学习器的权值，使其在表决中起到更大的作用，同时减小分类误差率大的基学习器的权值。

</br>

#### AdaBoost算法

- 对于基学习器$G_1(x)$，初始化所有训练样本的权值分布为$D_1$
  $$
  D_1=\left(w_{11},w_{12},\ldots,w_{1N}\right),\ w_{1i}=\dfrac{1}{N},
  $$
  即每个样本具有**相同的权值**。
- 对于使用权值$D_m$得到的基分类器$G_m(x)$，计算其在训练集上的**分类误差率**，即被$G_m(x)$误分类样本的权值和
  $$
  e_m=P(G_m(x_i)\ne y_i)=\sum_{i=1}^{N}{w_{m,i}\cdot I(G_m(x_i)\neq y_i)}.
  $$
- 计算$G_m(x)$在最终加权表决中的权重
  $$
  \color{red} \alpha_m=\dfrac{1}{2}\ln{\dfrac{1-e_m}{e_m}}.
  $$
  由于$e_m$减小会使得$\alpha_m$增大，所以**分类误差率越小的基分类器在最终的最用越大**。
- 更新权值分布
  $$
  D_{m+1}=\left(w_{m+1,1},\ldots,w_{m+1,N}\right),
  $$
  $$
  w_{m+1,i}=\dfrac{w_{m,i}\cdot\exp{(-\alpha_m y_i G_m(x_i))}}{Z_m}=\dfrac{w_{m,i}\cdot\exp{(-\alpha_m y_i G_m(x_i))}}{\sum_{i=1}^{N}{w_{m,i}\cdot\exp{(-\alpha_m y_i G_m(x_i))}}}.
  $$
  其中$Z_m$作为一个形成概率分布的**规范化因子**。
- 所有基分类器的线性组合加权表决作为最终的分类器
  $$
  \color{red} G(x)={\rm sign}(f(x))={\rm sign}\left(\sum_{m=1}^{M}{\alpha_m G_m(x)}\right).
  $$
- 不改变训练数据，而**不断改变训练数据权值的分布**，使训练数据在基学习器的学习中起到不同的作用，这是 AdaBoost 的一个特点。

</br>

#### AdaBoost的优缺点

- 优点</br>
  能够基于泛化性能相当弱的的学习器构建出很强的集成，分类精度高，**不容易发生过拟合**。
- 缺点</br>
  **对异常样本比较敏感**，异常样本在迭代过程中会获得较高的权值，影响最终学习器的性能表现。

</br>

#### 如何利用AdaBoost的权值

- 对于可以对特定数据分布进行学习的基学习器，可以通过**重赋权法**（re-weighting），例如可以**修改损失函数**，使得不同样本的损失值需要和权值相乘后生效。
- 对于无法接受带权样本的基学习算法，可以通过**重采样**（re-sampling）来产生训练集，这也是为什么之前说AdaBoost可以理解为基于概率分布来选取样本。详细做法是：10个样本中每个样本被抽中的概率是$w_{m,i}$，现在按抽取概率连续做10次可放回的样本抽取，得到训练集即可训练出一个分类器。

</br>

#### 为什么AdaBoost能快速收敛

每轮训练结束后，AdaBoost 会对样本的权重进行调整，调整的结果是越到后面被错误分类的样本权重会越高。而后面的分类器为了达到较低的带权分类误差，会把样本权重高的样本分类正确。这样造成的结果是，虽然每个弱分类器可能都有分错的样本，然而整个AdaBoost却能保证对每个样本进行正确分类，从而实现快速收敛。

</br>

#### 前向分步算法与AdaBoost

- AdaBoost算法是前向分步算法的特例。此时，基函数为基分类器，损失函数为指数函数$L(y,f(x))=\exp{(-y\ast f(x))}$。
Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而AdaBoost就是**加法模型+指数损失函数+前向分步算法**。

</br>

> [!NOTE|label:参考资料]
> [集成学习](http://pelhans.com/2019/09/18/ML_mianshi-note12/?utm_source=tuicool&utm_medium=referral)</br>
> [ML-专题-集成学习](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/C-%E4%B8%93%E9%A2%98-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.md)</br>