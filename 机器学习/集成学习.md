# 集成学习

</br>

#### 基本思想

通过**构建并结合多个学习器**来完成学习任务，组合成一个性能更好的学习器，有时也叫多分类器系统。

> [!TIP|label:集成学习为什么有效？]
> 不同的模型通常会在测试集上产生不同的误差。平均上，集成模型能至少与其任一成员表现一致；并且如果成员的误差是独立的，集成模型将显著地比其成员表现更好。

</br>

#### 学习策略

- 对于训练集数据，通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。也就是说，集成学习有两个主要的问题需要解决，第一是**如何得到若干个个体学习器**，第二是**如何选择一种结合策略**，将这些个体学习器集合成一个强学习器。
- 第一种就是所有的个体学习器都是一个种类的，或者说是**同质**的。比如都是决策树或者神经网络个体学习器。
- 第二种是所有的个体学习器不全是一个种类的，或者说是**异质**的。
- 目前来说，同质个体学习器的应用是最广泛的，一般常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。
- 同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类：
  1. 个体学习器之间**存在强依赖关系**，一系列个体学习器基本都需要串行生成，代表算法是Boosting系列算法；
  2. 个体学习器之间**不存在强依赖关系**，一系列个体学习器可以并行生成，代表算法是Bagging和随机森林（Random Forest）系列算法。

</br>

#### 集成学习有哪些基本步骤

集成学习一般可分为以下3个步骤：
1. 找到误差相互独立的基分类器；
2. 训练基分类器；
3. 合并基分类器的结果。

</br>

#### 常见的结合策略

- 平均法
  1. 对于数值类**回归预测问题**，最常见的结合策略是平均法，分为：简单平均法和加权平均法。
  2. 加权平均法的权重是每个个体学习器的权重。
  3. 在个体学习器性能相差较大时宜采用加权平均法，性能相近时宜采用简单平均法。
- 投票法
  1. 对于**分类预测问题**，通常使用的是投票法。
  2. **相对多数投票法**，也就是少数服从多数，即`T`个弱学习器的对样本$x$的预测结果中，数量最多的类别$c_i$为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。
  3. **绝对多数投票法**，也就是票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数，否则会拒绝预测。
  4. **加权投票法**，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。
- 学习法
  1. 代表方法是Stacking，当使用Stacking的结合策略时，不是对弱学习器的结果做简单的逻辑处理，而是**再加上一层学习器**。也就是说，将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。
  2. 在这种情况下，将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。

</br>

#### Boosting方法

- 基于**串行策略**</br>
  基学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。
- 基本思路
  1. 先从初始训练集训练一个基学习器；初始训练集中各样本的权重是相同的；
  2. 根据上一个基学习器的表现，**调整样本权重**，使分类错误的样本得到更多的关注；
  3. 基于调整后的样本分布，训练下一个基学习器；
  4. 测试时，对各基学习器加权得到最终结果。
- 特点是每次学习都会使用全部训练样本
- 代表算法包括AdaBoost、GBDT、XGBoost。
- 两个基本问题
  1. 每一轮如何改变数据的权值或概率分布？
  2. 如何将弱分类器组合成一个强分类器？

</br>

#### Bagging方法

- 基于**并行策略**</br>
  基学习器之间不存在依赖关系，可同时生成。
- 基本思路
  1. 利用自助采样法对训练集随机采样，重复进行`T`次；
  2. 基于每个采样集训练一个基学习器，并得到`T`个基学习器；
  3. 预测时，集体投票决策。
- 自助采样法：对`m`个样本的训练集，有放回的采样`m`次；此时，样本在`m`次采样中始终没被采样的概率约为`0.368`，即每次自助采样只能采样到全部样本的`63%`左右。
- 特点是训练每个基学习器时**只使用一部分样本**，并且偏好使用**不稳定的学习器**作为基学习器（所谓不稳定的学习器，指的是对样本分布较为敏感的学习器）。
- 代表算法包括随机森林。

</br>

#### Stacking方法简述

- 基于**串行策略**</br>
  初级学习器与次级学习器之间存在依赖关系，初级学习器的输出作为次级学习器的输入。
- 基本思路：
  1. 先从初始训练集训练`T`个不同的初级学习器；
  2. 利用每个初级学习器的输出构建一个次级数据集，该数据集依然使用初始数据集的标签；
  3. 据新的数据集训练次级学习器；
  4. 多级学习器的构建过程类似。
- Stacking方法也可以作为一种**结合策略**，比如加权平均和投票都属于结合策略。
- 特点是为了降低过拟合的风险，一般会利用**交叉验证**的方法使不同的初级学习器在不完全相同的子集上训练。


</br>

> [!NOTE|label:参考资料]
> [集成学习](http://pelhans.com/2019/09/18/ML_mianshi-note12/?utm_source=tuicool&utm_medium=referral)</br>
> [ML-专题-集成学习](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/C-%E4%B8%93%E9%A2%98-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.md)</br>