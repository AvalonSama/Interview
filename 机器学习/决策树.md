# 决策树

</br>

#### 什么是决策树

决策树是一种分类和回归的基本模型，可从三个角度来理解它，即：
1. 一棵树。
2. if-then规则的集合，该集合是决策树上的所有从根节点到叶节点的路径的集合。
3. 定义在**特征空间与类空间上的条件概率分布**。决策树实际上是将特征空间划分成了互不相交的单元，每个从根到叶的路径对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。实际中，哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别。

</br>

#### 决策树的基本思想

实际上就是寻找最纯净的划分方法，这个最纯净在数学上叫纯度，纯度通俗点理解就是目标变量要分得足够开（$y=1$的和$y=0$的混到一起就会不纯）。另一种理解是分类误差率的一种衡量。实际决策树算法往往用到的是不纯度。不纯度的选取有多种方法，每种方法也就形成了不同的决策树方法，比如`ID3`算法使用**信息增益**作为不纯度；`C4.5`算法使用**信息增益比**作为不纯度；`CART`（分类与回归树）算法使用**基尼系数**作为不纯度。

</br>

#### 优缺点

- 优点：
  1. 决策树算法中学习简单的决策规则建立决策树模型的过程非常容易理解。
  2. 决策树模型可以可视化，非常直观。
  3. 应用范围广，可用于分类和回归，而且非常容易做多类别的分类。
  4. 能够处理数值型和连续的样本特征。
  5. 分类速度快。
- 缺点：
  1. 很容易在训练数据中生成复杂的树结构，**造成过拟合**。剪枝可以缓解过拟合的负作用，常用方法是限制树的高度、叶子节点中的最少样本数量。
  2. 学习一棵最优的决策树被认为是NP-Complete问题。实际中的决策树是基于启发式的贪心算法建立的，这种算法不能保证建立全局最优的决策树。Random Forest 引入随机能缓解这个问题。

</br>

#### 训练的目标

- 决策树学习的本质上是从训练数据集中归纳出一组分类规则，使它与训练数据矛盾较小的同时具有较强的泛化能力。
- 决策树学习的损失函数通常是**正则化的极大似然函数**。
- 决策树学习的策略是以损失函数为目标函数的最小化。
- 由于这个最小化问题是一个NP完全问题，现实中，通常采用启发式算法（SMO算法）来近似求解这一最优化问题，得到的决策树是次最优的。

</br>

#### 训练的组成

- 决策树的训练通常由三部分组成：**特征选择、树的生成、剪枝**。
- 从总体上看，这三个步骤就是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这个过程就是划分特征空间，构建决策树的过程。

</br>

#### 特征选择的方法
特征选择在于选取对训练数据具有分类能力的特征，提高决策树的效率。通常的准则是**信息增益**或**信息增益比**。

</br>

#### ID3算法和信息增益

- ID3算法的核心是在决策树各个节点上应用**信息增益**准则选择特征。从根节点开始递归，对节点计算所有可能的特征的信息增益，选择信息增益**最大**的特征作为节点的特征，由该特征的不同取值建立子节点。
- 递归的终止条件通常有两个，一是所有训练数据子集被基本正确分类；二是没有合适的特征可选，即可用特征为`0`，或者可用特征的信息增益或信息增益比都很小了。
- 信息增益表示的是得知特征$X$的信息而使得类$Y$的不确定性减少的程度。
- 随机变量$X$的概率分布$P(X=x_i)=p_i$，其熵定义为
  $$
  H(X)=-\sum_{i=1}^{n}{p_i\log{p_i}}.
  $$
  熵是对随机变量不确定性的度量，也可以说是对随机变量的概率分布的一个衡量。熵越大，则随机变量的不确定性越大。对同一个随机变量，当它的概率分布为均匀分布时，不确定性最大，熵也最大。对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大。
- **条件熵**$H(Y|X)$表示在已知随机变量$X$的条件下，随机变量$Y$的不确定性，定义为
  $$
  H(Y|X)=\sum_{i=1}^{n}{p_iH(Y|X=x_i)}.
  $$
- **特征$A$对训练数据集$D$的信息增益是信息熵和条件熵的差**：$\color{red} g(D,A)=H(D)-H(D|A)$，其中
  $$
  \begin{aligned}
  &H(D)=-\sum_{k=1}^K{\dfrac{|C_k|}{|D|}\log{\dfrac{|C_k|}{|D|}}} \\
  &H(D|A)=\sum_{i=1}^n{\dfrac{|D_i|}{|D|}H(D_i)},
  \end{aligned}
  $$
  其中$C_k$表示数据集中类别为$k$的子集，$D_i$表示数据集中特征$A$取值为$a_i$的子集。

> [!NOTE|label:ID3的缺点]
> - 没有考虑连续特征；
> - 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题；
> - 对缺省值的情况没有考虑；
> - 没有考虑过拟合的问题。

</br>

#### C4.5和信息增益比

- 信息增益比也可能产生一个问题就是，**对可取数值数目较少的属性有所偏好**。因此，C4.5算法并不是直接选择使用信息增益比最大的候选划分属性，而是使用了一个启发式算法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益比最高的。
- 特征A对训练数据集$D$的**信息增益比**是其信息增益$g(D,A)$与训练集关于特征$A$的熵之比
  $$
  \color{red} g_R(D,A)=\dfrac{g(D,A)}{H_A(D)},
  $$
  其中$H_A(D)=-\sum_{i=1}^{n}{\dfrac{|D_i|}{|D|}\log{\dfrac{|D_i|}{|D|}}}$，$n$是特征$A$取值的个数。

</br>

#### 损失函数

- 设树$T$的叶子节点个数为$|T|$，某一叶子节点$t$有$N_t$个样本点，其中$k$类的有$N_{tk}$个，则损失函数可以定义为：$\color{red} C_\alpha(T)=C(T)+\alpha|T|$，其中
  $$
  C(T)=\sum_{t=1}^{|T|}{N_tH_t(T)},
  $$
  $C(T)$表示模型对训练数据的预测误差，$|T|$表示模型复杂度。其中$t$上的经验熵
  $$
  H_t(T)=-\sum_{k}{\dfrac{N_{tk}}{N_t}\log{\dfrac{N_{tk}}{N_t}}},
  $$
- 参数$\alpha\ge0$控制预测误差和模型复杂度之间的影响，较大的$\alpha$促使选择较简单的树，较小的的$\alpha$促使选择较复杂的树，而$\alpha=0$意味着只考虑模型与训练数据的拟合程度。

</br>

#### 过拟合

- 对训练数据预测效果很好，但是**测试数据预测效果较差**，则称出现了过拟合现象。
- 对于过拟合现象产生的原因，有以下几个方面：
  1. 在决策树构建的过程中，对决策树的生长**没有进行合理的限制**（剪枝）；
  2. 在建模过程中使用了**较多的输出变量**，变量较多也容易产生过拟合；
  3. 样本中有一些**噪声数据**，噪声数据对决策树的构建的干扰很多，没有对噪声数据进行有效的剔除。
- 对于过拟合现象的预防措施，有以下一些方法：
  1. 选择合理的参数进行**剪枝**，一般用后剪枝的方法来做；
  2. K-folds**交叉验证**，将训练集分为K份，然后进行K次的交叉验证；
  3. **减少特征**，计算每一个特征和响应变量的相关性，常见的为皮尔逊相关系数，将相关性较小的变量剔除，当然还有一些其他的方法来进行特征筛选，比如基于决策树的特征筛选，通过正则化的方式来进行特征选取等。

</br>

#### 剪枝

- 剪枝是决策树学习算法对付**过拟合**的主要手段。
- 决策树的剪枝往往通过极小化决策树整体的损失函数来实现，当$\alpha$确定时，选择损失函数最小的模型。
- 假设一组叶子节点回缩到其父节点之前与之后的决策树损失分别为$C_\alpha(T_A)$和$C_\alpha(T_B)$，若存在$\color{red} C_\alpha(T_A)\le C_\alpha(T_B)$，则进行剪枝。
- 剪枝算法可以由一种动态规划的算法实现。

</br>

#### 预剪枝和后剪枝

- **预剪枝**</br>
  在决策树生成过程中，对每个节点**在划分前先进行估计**，若当前的节点划分不能带来决策树泛化的提升（剪枝前和剪枝后精度的比较），则停止划分并将当前节点标记为叶子节点。
- **后剪枝**</br>
  先从训练数据集中生成一棵完整的决策树，然后**自底向上对非叶子节点进行考察**，若将该节点对应的子树替换为叶子结点能够带来决策树泛化能力的提升，则将该节点替换为叶子节点。
- 后剪枝和预剪枝的优缺点</br>
  后剪枝决策树通常比预剪枝决策树保留更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛化能力往往优于预剪枝。但后剪枝决策树训练开销比预剪枝大得多。

> [!TIP|label:Python中剪枝相关的参数]
> `max_depth`：树的高度</br>
> `min_samples_split`：叶子结点的数目</br>
> `max_leaf_nodes`：最大叶子节点数</br>
> `min_impurity_split`：限制不纯度

</br>

#### 如何处理缺失值

- 使用某一属性上**无缺失值的样本子集**来计算信息增益，并乘上无缺失值的样本比例。
- 每一个缺失该属性的样本会同时进入到所有分叉中，但样本权重从`1`调整为该分叉中**无缺失值的样本数在无缺失值的样本子集中所占的比例**。
- 如果测试样本中也存在缺失值，如果有专门处理缺失值的分支，就走这个分支；或者从属性最常用的分支走；也可以同时探查所有的分支，然后算每个类别的概率，取概率最大的类别赋值给该样本（C4.5）。

</br>

#### CART基本概念

- CART算法是在给定输入随机变量X条件下输出随机变量`Y`的**条件概率分布**的学习方法。
- CART算法假设决策树是**二叉树**，内部节点特征的取值为“是”和“否”。
- CART决策树等价于递归地二分每个特征，将输入空间或特征空间划分为**有限个单元**，然后在这些单元上确定在输入给定的条件下输出的条件概率分布。
- CART决策树既可以用于**分类**，也可以用于**回归**。
- CART对**回归树**用**平方误差最小化准则**来选择特征，对**分类树**用**基尼指数最小化准则**选择特征。

</br>

#### CART回归树

- 一个回归树对应着输入空间或特征空间的一个划分以及在划分单元上的输出值。
- 假设已将输入空间划分为$M$个单元$\{R_1,\ldots,R_M\}$，并在每个单元$R_m$上对应有输出值$c_m$，则该回归树可表示为
  $$
  f(x)=\sum_{m=1}^{M}{c_mI(x\in R_m)},
  $$
  其中$I$为指示函数。
- 如果已经划分好了输入空间，通常使用**平方误差**
  $$
  {\sum_{x_i\in R_m}\left(y_i-f(x_i)\right)}^2,
  $$
  作为损失函数来表示回归树对于训练数据的预测误差，通过最小化损失函数来求解每个划分单元的最优输出值。
- 如果使用平方误差，易知**最优输出值**即每个划分单元上所有实例的均值${\hat{c}}_m={\rm avg}(y_i|x_i\in R_m)$。


</br>

> [!NOTE|label:参考资料]
> [决策树](https://www.jianshu.com/p/eee203a1ad11)</br>
> [决策树(一)——构造决策树方法](https://blog.csdn.net/u012328159/article/details/70184415)</br>
> [决策树（四）——缺失值处理](https://blog.csdn.net/u012328159/article/details/79413610)</br>