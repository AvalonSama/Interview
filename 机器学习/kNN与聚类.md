# kNN与聚类

</br>

#### k近邻法

- k近邻法（k-Nearest Neighbor, kNN）是一种基本**分类与回归方法**，而不是聚类算法。
- 其算法基本思想是，给定测试实例，基于某种**距离度量**找出训练集中与其**最靠近的`k`个实例点**，然后基于这`k`个最近邻的信息来进行预测。
- 流程描述：
  1. 计算**测试数据**与各个**训练数据**之间的距离；
  2. 按照距离的**递增**关系进行排序；
  3. 选取**距离最小**的`k`个点；
  4. 确定这`k`个点所在类别的**出现频率**；
  5. 返回这`k`个点中出现频率**最高**的类别作为测试数据的预测分类。
- **三个基本要素：**k值的选择、距离度量、分类决策规则。
- 在分类任务中可使用**投票法**，即选择这`k`个实例中出现最多的标记类别作为预测结果；</br>
  在回归任务中可使用**平均法**，即将这`k`个实例的实值输出标记的平均值作为预测结果；</br>
  还可基于距离远近进行**加权平均或加权投票**，距离越近的实例权重越大。

<div align=center><img src="./img/kNN.png" width=300px/></div>

</br>

#### kNN中的距离度量

- 特征空间中的**两个实例点的距离是两个实例点相似程度的反映**。k近邻法的特征空间一般是`n`维实数向量空间，使用的距离是欧氏距离。
- 定义向量$x_i=\left(x_i^{(1)},\ldots,x_i^{(n)}\right)^T$和$x_j=\left(x_j^{(1)},\ldots,x_j^{(n)}\right)^T$，向量间的闵可夫斯基距离$L_p$距离定义为
  $$
  L_p(x_i,x_j)=\left(\sum_{l=1}^n\left|x_i^{l}-x_j^{l}\right|^p\right)^\frac{1}{p},
  $$
  其中$p\geq1$。
  1. 当$p=1$时，称为**曼哈顿距离**
    $$
    L_1(x_i,x_j)=\sum_{l=1}^n\left|x_i^{l}-x_j^{l}\right|.
    $$
  2. 当$p=2$时，称为**欧氏距离**
    $$
    \color{red} L_2(x_i,x_j)=\left(\sum_{l=1}^n\left|x_i^{l}-x_j^{l}\right|^2\right)^\frac{1}{2}.
    $$
  3. 当$p=3$时，为**各个坐标距离的最大值**
    $$
    L_\infty(x_i,x_j)=\max_l\left|x_i^{l}-x_j^{l}\right|.
    $$

</br>

#### kNN中`k`的取值

- 如果`k`的取值过小时，一旦有噪声得成分存在将会对预测产生比较大影响，例如取`k`值为`1`时，一旦最近的一个点是噪声，那么就会出现偏差。`k`值的减小就意味着整体模型变得复杂，容易发生过拟合。
- 如果`k`的取值过大时，就相当于用较大邻域中的训练实例进行预测，学习的近似误差会增大。这时与输入目标点较远实例也会对预测起作用，使预测发生错误。`k`值的增大就意味着整体的模型变得简单。
- 常用的方法是从`k = 1`开始，使用**验证集**估计分类器的误差率。重复该过程，每次`k`增值`1`，允许增加一个近邻。选取产生最小误差率的`k`。
- 从经验上看，一般`k`的取值不超过`20`，**上限是`n`的开方**。随着数据集的增大，`k`的值也要增大。

</br>

#### K-Means聚类算法

- K-Means算法又名K均值算法，是一个**无监督的聚类算法**，试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“**簇**”（cluster）。过这样的划分，每个簇可能对应于一些潜在的概念或类别。
- 其算法基本思想是：通过不断调整`K`个簇的簇中心位置，**让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大**。
- 流程描述：
  1. 先从样本集中**随机选取`K`个样本**$\{\mu_1,\ldots,\mu_K\}$作为簇中心（质心向量）；
  2. 计算每一个样本$x_i$与这`K`个簇中心$\mu_j$的距离${\|x_i-\mu_j\|}_2^2$，将其划分到**与其距离最近的簇中心**所在的簇（类别）中；
  3. 对于每一个簇，**计算新的簇中心**$\hat{\mu_j}=\dfrac{1}{N_j}\sum x_i$；
  4. 如果所有簇中心都不再发生变化，或满足最大运行轮数或最小调整幅度阈值，结束。

<div align=center><img src="./img/K-Means.jpeg" width=350px/></div>

</br>

#### K-Means中`K`的取值

- **手肘法**
  - 核心指标是**误差平方和**SSE（Sum of the Squared Errors）
    $$
    {\rm SSE}=\sum_{i=1}^k\sum_{x\in C_i}{|x-\mu_i|}^2
    $$
    其中$C_i$是第`i`个簇，$x$是$C_i$中的样本点，$\mu_i$是簇中心（质心/均值向量）。SSE可以衡量聚类误差。
  - 核心思想是：随着聚类数`K`的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。在这个过程中，当`K`小于真实聚类数时，**SSE的下降幅度会很大**，而当`K`到达真实聚类数时，再增加`K`所得到的聚合程度回报会迅速变小，所以**SSE的下降幅度会骤减**，进而趋于平缓，也就是说SSE和`K`的关系图是一个手肘的形状，而这个肘部对应的`K`值就是数据的真实聚类数。

<div align=center><img src="./img/手肘法.png" width=400px/></div>

- **轮廓系数**
  - 核心指标是**轮廓系数**（Silhouette Coefficient），某个样本点$x_i$的轮廓系数定义如下：
    $$
    S=\dfrac{b-a}{\max(a,b)}
    $$
    其中，$a$是$x_i$与**同簇的其它样本的平均距离**（凝聚度），$x_i$与其它簇的距离表示为与该簇中所有样本的平均距离，其中$b$表示$x_i$**与最近的一个簇的距离**（分离度）。
  - 平均轮廓系数的取值范围为`[-1, 1]`，且**簇内样本的距离越近，簇间样本距离越远**，平均轮廓系数越大，聚类效果越好。那么，平均轮廓系数最大的`K`便是最佳聚类数。

</br>

#### K-Means的优点和缺点

- 优点：
  1. 原理比较简单，实现也是很容易，收敛速度快；
  2. 聚类效果较优；
  3. 算法的**可解释度比较强**；
  4. 主要需要调参的参数仅仅是簇数`K`。
- 缺点：
  1. **`K`值的选取不好把握**；
  2. 对于非凸的数据集比较难收敛；
  3. 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。
  4. 采用迭代方法，得到的结果只是局部最优。
  5. **对噪音和异常点比较的敏感。**

</br>

#### K-Means++算法

- 由于K-Means算法的分类结果会**受到初始点的选取而有所区别**，因此K-Means++是一种改进算法。它仅仅改变了初始聚类中心（质心）的选择，基本思路就是使初始的质心之间的**相互距离要尽可能的远**。
- 流程描述：
  1. 随机选取一个样本点作为第一个聚类中心；
  2. 计算每一个样本与当前所有聚类中心的**最短距离**（即与最近一个聚类中心的距离），选择一个距离最大的点作为又一个聚类中心；
  3. 重复直至选出`K`个聚类中心；
  4. 利用这`K`个质心来作为初始化质心去运行标准的K-Means算法。

</br>

#### DBSCAN聚类算法

</br>

> [!NOTE|label:参考资料]
> [机器学习（二）：k近邻法（kNN）](https://blog.csdn.net/eeeee123456/article/details/79927128)</br>
> [kNN算法：K最近邻分类算法](https://www.cnblogs.com/jyroy/p/9427977.html)</br>
> [K-Means聚类算法原理](https://www.cnblogs.com/pinard/p/6164214.html)</br>
> [K-means聚类最优k值的选取](https://blog.csdn.net/qq_15738501/article/details/79036255)</br>
> [K-means与K-means++](https://www.cnblogs.com/wang2825/articles/8696830.html)

