# 支持向量机

</br>

#### 什么是支持向量机

- 支持向量机（Support Vector Machines, SVM）是一种**二分类模型**。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使其成为实质上的**非线性分类器**。
- SVM的学习策略就是间隔最大化，可形式化为一个**求解凸二次规划的问题**，也等价于正则化的合页损失函数的最小化问题。
- SVM的最优化算法是求解凸二次规划的最优化算法。
- 对于训练集$T=\{(x_1,y_1),\ldots,(x_N,y_N)\}$，标签$y_i\in\{+1,-1\}$，支持向量机期望寻找分类超平面$w^Tx+b=0$。

> [!TIP|label:什么是支持向量]
> 训练数据集中**与分离超平面距离最近的样本点**的实例称为支持向量。

</br>

#### SVM的分类

- **线性可分支持向量机**（硬间隔支持向量机）</br>
  当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
- **线性支持向量机**（软间隔支持向量机）</br>
  当训练数据接近线性可分时，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。
- **非线性支持向量机**</br>
  当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

</br>

#### 函数间隔和几何间隔

- 一个点距离分离超平面的远近可以表示分类预测的**确信程度**。
- 在分类超平面确定的情况下，$|wx+b|$能够相对地表示点$x$距离超平面的远近。而$w^Tx+b$与标签$y$的符号是否一致能够表示分类是否正确。因此**函数间隔表示了分类的正确性和确信度**
  $$
  {\hat{\gamma}}_i=y_i(w^Tx_i+b).
  $$
  而超平面$(w,b)$关于某一训练集$T$的函数间隔指的是所有样本点中函数间隔的最小值
  $$
  \hat{\gamma}=\min{{\hat{\gamma}}_i}.
  $$
- 如果对超平面的法向量$w$增加约束，**使得间隔不随超平面的缩放而变化，引入几何间隔**
  $$
  \gamma_i=\dfrac{y_i}{||w||}(w^Tx_i+b)=\dfrac{\gamma_i}{||w||}.
  $$

</br>

#### 为什么要采用间隔最大化

- 当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。利用间隔最大化求得**最优分离超平面**，这时，**解是唯一的**。另一方面，此时的分隔超平面所产生的分类结果对未知实例的**泛化能力最强**。
- 不仅要将正负样例点分开，而且对于最难分的点也要有足够大的确信度把它们分开。

</br>

#### 最大间隔分离超平面
- 最大间隔分类超平面中的“间隔”指的是**几何间隔**。
- 目标函数可以定义为$\max{\gamma}$，可以导出为一个**约束最优化问题**
  $$
  \begin{aligned}
  &\max_{w,b}\gamma \\
  &{\rm s.t.}\ \ y_i\dfrac{f(x_i)}{||w||}\ge\gamma,\ i=1,\ldots,N.
  \end{aligned}
  $$
- 根据几何间隔和函数间隔的关系$\gamma=\dfrac{\hat{\gamma}}{||w||}$，若令函数间隔$\hat{\gamma}=1$（方便推导和优化），可以将问题改写为
  $$
  \begin{aligned}
  &\max_{w,b}\dfrac{1}{||w||}\\
  &{\rm s.t.}\ \ y_i(w^Tx_i+b)\ge1,\ i=1,\ldots,N.
  \end{aligned}
  $$
- 由于求解$\dfrac{1}{||w||}$的最大值相当于求解$\dfrac{1}{2}{||w||}^2$的最小值，上述问题等价于
  $$
  \begin{aligned}
  &\min_{w,b}\dfrac{1}{2}{||w||}^2 \\
  &{\rm s.t.}\ \ y_i(w^Tx_i+b)-1\ge0,\ i=1,\ldots,N.
  \end{aligned}
  $$
  由于目标函数是二次的，约束条件是线性的，所以它是一个**凸二次规划问题**，即约束最优化问题。
- 线性可分训练数据集的最大间隔分离超平面是**存在且唯一**的。

</br>

#### 对偶问题

- 约束最优化问题可以利用**拉格朗日对偶性**通过变换得到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法。
- 简单来讲，通过给每一个约束条件加上一个拉格朗日乘子$\alpha_i$（Lagrange multiplier），定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出问题）
  $$
  L(w,b,\alpha)=\frac{1}{2}{||w||}^2-\sum_{i=1}^{N}{\alpha_i\left(y_i(w^Tx_i+b)-1\right)}.
  $$
  问题的目的是寻找$\alpha_i\ge0$使得目标函数尽可能大，若存在不满足$y_i(w^Tx_i+b)-1\ge0$的情况，只需要将$\alpha_i$取无穷大即可最大化$L(w,b,\alpha)$。在要求约束条件得到满足的情况下最小化$\frac{1}{2}{||w||}^2$，实际上等价于最小化$\max_\alpha L(w,b,\alpha)$
  $$
  \min_{w,b}\max_\alpha L(w,b,\alpha).
  $$
  根据最小化根据拉格朗日对偶性，原始问题的**对偶问题**是极大极小问题
  $$
  \max_\alpha\min_{w,b} L(w,b,\alpha).
  $$
- 原始问题和对偶问题等价需要满足**KKT条件**，它是一个非线性规划问题能有最优化解法的必要和充分条件。

</br>

#### 为什么要引入对偶问题

- 对偶问题往往**更容易求解**</br>
  当寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。
- 目前处理的模型**严重依赖于数据集的维度`d`**，如果维度`d`太高就会严重提升运算时间；对偶问题把SVM从依赖`d`个维度转变到依赖`N`个数据点，最后计算时只有支持向量有意义，所以计算量比`N`小很多。
- 方便**引入核函数**，进而推广到非线性分类问题。

</br>

#### 求解对偶问题

- 固定$\alpha$，要让目标函数关于$w$和$b$最小化，对两者分别**求偏导并令偏导为零**
  $$
  \begin{aligned}
  &\partial_w L=w-\sum_{i=1}^{N}{\alpha_iy_ix_i}=0 \to w=\sum_{i=1}^{N}{\alpha_iy_ix_i},\\
  &\partial_b L=-\sum_{i=1}^{N}{\alpha_iy_i}=0.
  \end{aligned}
  $$
  把结果代回原式
  $$
  \begin{aligned}
  L(w,b,\alpha)&=\dfrac{1}{2}w^Tw-\sum_{i=1}^{N}{\alpha_iy_iw^Tx_i}-\sum_{i=1}^{N}{\alpha_iy_ib}+\sum_{i=1}^{N}{\alpha_i} \\
  &=-\dfrac{1}{2}{w^T}\sum_{i=1}^{N}{\alpha_iy_ix_i}-b\sum_{i=1}^{N}{\alpha_iy_i}+\sum_{i=1}^{N}{\alpha_i} \\
  &=\sum_{i=1}^{N}\alpha_i-\dfrac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha_i\alpha_jy_iy_jx_i^Tx_j}.
  \end{aligned}
  $$
- 上一步得到的$L(w,b,\alpha)$实际已经只包含一个变量$\alpha$，求对$\alpha$的极大，也就是一个关于对偶问题的最优化问题
  $$
  \begin{aligned}
  &\max_\alpha{\sum_{i=1}^{N}\alpha_i-\dfrac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha_i\alpha_jy_iy_jx_i^Tx_j}} \\
  &{\rm s.t.}\ \ \sum_{i=1}^{N}{\alpha_iy_i}=0\ \&\ \alpha_i\ge0,\ i=1,2,\ldots,N.
  \end{aligned}
  $$
  转化为对偶问题（约束最优化问题）
  $$
  \min_\alpha{\dfrac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha_i\alpha_jy_iy_jx_i^Tx_j}-\sum_{i=1}^{N}\alpha_i},
  $$
  这样就可以计算出最优解$\alpha^\ast=\left(\alpha_1^\ast,\alpha_2^\ast,\ldots,\alpha_N^\ast\right)^T$。
- 从而计算其它参数
  $$
  w^\ast=\sum_{i=1}^{N}{\alpha_i^\ast y_ix_i},
  $$
  可以选择某一个正分量$\alpha_j^\ast>0$，计算
  $$
  b^\ast=y_j-\sum_{i=1}^{N}{\alpha_i^\ast y_i\left(x_i^Tx_j\right)}.
  $$
  从而得到分离超平面${w^\ast}^Tx+b^\ast=0$，以及分类决策函数$f(x)={\rm sign}({w^\ast}^Tx+b^\ast)$。

</br>

> [!NOTE|label:参考资料]
> [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_JULY_v/article/details/7624837#commentBox)</br>
> [SVM之面试常问问题](https://blog.csdn.net/Jum_Summer/article/details/80793835)
>