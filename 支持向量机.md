# 支持向量机

</br>

#### 什么是支持向量机

- 支持向量机（Support Vector Machines, SVM）是一种**二分类模型**。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使其成为实质上的**非线性分类器**。
- SVM的学习策略就是间隔最大化，可形式化为一个**求解凸二次规划的问题**，也等价于正则化的合页损失函数的最小化问题。
- SVM的最优化算法是求解凸二次规划的最优化算法。
- 对于训练集$T=\{(x_1,y_1),\ldots,(x_N,y_N)\}$，标签$y_i\in\{+1,-1\}$，支持向量机期望寻找分类超平面$w^Tx+b=0$。

> [!TIP|label:什么是支持向量]
> 训练数据集中**与分离超平面距离最近的样本点**的实例称为支持向量。

</br>

#### SVM的分类

- **线性可分支持向量机**（硬间隔支持向量机）</br>
  当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
- **线性支持向量机**（软间隔支持向量机）</br>
  当训练数据接近线性可分时，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。
- **非线性支持向量机**</br>
  当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

</br>

#### 函数间隔和几何间隔

- 一个点距离分离超平面的远近可以表示分类预测的**确信程度**。
- 在分类超平面确定的情况下，$|wx+b|$能够相对地表示点$x$距离超平面的远近。而$w^Tx+b$与标签$y$的符号是否一致能够表示分类是否正确。因此**函数间隔表示了分类的正确性和确信度**
  $$
  {\hat{\gamma}}_i=y_i(w^Tx_i+b).
  $$
  而超平面$(w,b)$关于某一训练集$T$的函数间隔指的是所有样本点中函数间隔的最小值
  $$
  \hat{\gamma}=\min{{\hat{\gamma}}_i}.
  $$
- 如果对超平面的法向量$w$增加约束，**使得间隔不随超平面的缩放而变化，引入几何间隔**
  $$
  \gamma_i=\dfrac{y_i}{||w||}(w^Tx_i+b)=\dfrac{\gamma_i}{||w||}.
  $$

</br>

#### 为什么要采用间隔最大化

- 当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。利用间隔最大化求得**最优分离超平面**，这时，**解是唯一的**。另一方面，此时的分隔超平面所产生的分类结果对未知实例的**泛化能力最强**。
- 不仅要将正负样例点分开，而且对于最难分的点也要有足够大的确信度把它们分开。

</br>

#### 最大间隔分离超平面
- 最大间隔分类超平面中的“间隔”指的是**几何间隔**。
- 目标函数可以定义为$\max{\gamma}$，可以导出为一个**约束最优化问题**
  $$
  \begin{aligned}
  &\max_{w,b}\gamma \\
  &{\rm s.t.}\ \ y_i\dfrac{f(x_i)}{||w||}\e\gamma,i=1,2,…,N.
  \end{aligned}
  $$
根据几何间隔和函数间隔的关系\gamma=γw，若令函数间隔\hat{\gamma}=1（方便推导和优化），可以将问题改写为
maxw,b1w
s.t.\ \ y_i\left(w^Tx_i+b\right)\geq1,\ \ i=1,2,\ldots,N.
由于求解 1w的最大值相当于求解\frac{1}{2}w2的最小值，上述问题等价于
minw,b12w2
s.t.\ \ y_i\left(w^Tx_i+b\right)-1\geq0,\ \ i=1,2,\ldots,N.\bigm

  $$

</br>

> [!NOTE|label:参考资料]
> [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_JULY_v/article/details/7624837#commentBox)
>